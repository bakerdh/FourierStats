---
title: "Statistical analysis of periodic data in neuroscience"
author: "Daniel H. Baker"
date: "13/12/2020"
output:
  bookdown::pdf_document2:
    fig_caption: yes 
    toc: false   
    keep_tex: true
  html_document:
    df_print: paged
  pdf_document: default
bibliography: bibliography.bib
biblio-style: apalike
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# reasonably compact code to check which packages are installed, 
# install the missing ones, and activate all

# devtools::install_github("bakerdh/FourierStats")
# devtools::install('FourierStats')

packagelist <- c('bookdown','pals','shape','diagram','HDMD','FourierStats')
missingpackages <- packagelist[!packagelist %in% installed.packages()[,1]]
if (length(missingpackages)>0){install.packages(missingpackages)}
toinstall <- packagelist[which(!packagelist %in% (.packages()))]
invisible(lapply(toinstall,library,character.only=TRUE))

nsims <- 10000  # global value for the whole script - increase to 100000 for final run

# palette using the main colours from the NBDT journal logo
nbdtpal <- c(rgb(0,0,0),rgb(0.73,0.16,0.42),rgb(0.22,0.44,0.64),'darkgreen','darkorange')

v4Interp <- function(df, xo, yo, rmax = .75, gridRes = 67) {
  ## Create a function to perform Matlab's v4 interpolation.
  ## Takes as input a data-frame with columns x, y, and z (x co-ordinates, y co-ordinates, and amplitude)
  ## and variables xo and yo, the co-ordinates which will be use to create a grid for interpolation
  xo <- matrix(rep(xo,length(yo)),nrow = length(xo),ncol = length(yo))
  yo <- t(matrix(rep(yo,length(xo)),nrow = length(yo),ncol = length(xo)))
  xy <- df$x + df$y*sqrt(as.complex(-1))
  d <- matrix(rep(xy,length(xy)),nrow = length(xy), ncol = length(xy))
  d <- abs(d - t(d))
  diag(d) <- 1
  g <- (d^2) * (log(d)-1)   # Green's function.
  diag(g) <- 0
  weights <- qr.solve(g,df$z)
  xy <- t(xy)
  outmat <- matrix(nrow = gridRes,ncol = gridRes)
  for (i in 1:gridRes){
    for (j in 1:gridRes) {
      test4 <- abs((xo[i,j] + sqrt(as.complex(-1))*yo[i,j]) - xy)
      g <- (test4^2) * (log(test4)-1)
      outmat[i,j] <- g %*% weights}}
  outDf <- data.frame(x = xo[,1],outmat)
  names(outDf)[1:length(yo[1,])+1] <- yo[1,]
  return(outDf)}

addalpha <- function(col, alpha=1){apply(sapply(col, col2rgb)/255, 2, function(x) rgb(x[1], x[2], x[3], alpha=alpha))}

```

# Abstract

Many experimental paradigms in neuroscience involve driving the nervous system with periodic sensory stimuli. Neural signals recorded with a variety of techniques will then include phase-locked oscillations at the stimulation frequency. The analysis of such data often involves standard univariate statistics such as T-tests, conducted on the Fourier amplitude components. However, the assumptions of these tests will often be violated because amplitudes are not normally distributed, and furthermore weak signals might be missed if the phase information is discarded. An alternative approach is to conduct multivariate statistical tests using the real and imaginary Fourier components. Here we compare the performance of two multivariate extensions of the T-test: Hotelling's $T^2$ and a variant called $T^2_{circ}$. We develop a novel test of the assumptions of $T^2_{circ}$ based on the condition index, and suggest a heuristic for excluding outliers using the Mahanalobis distance. We then extend the statistic to multi-level designs, and propose a new statistical test termed $ANOVA^2_{circ}$. This has identical assumptions to $T^2_{circ}$, and is shown to be more sensitive than MANOVA when these assumptions are met. We demonstrate how these tests might be applied to two publicly available empirical data sets, and suggest concrete guidance for choosing which test to run. Implementations of these novel tools are provided as _R_ functions, and we hope that their wider adoption will improve the sensitivity of statistical inferences involving periodic data.

# Background

A widely used paradigm in many branches of neuroscience is to drive the nervous system using periodic stimuli. This entrains neural responses at the stimulation frequency, resulting in high signal-to-noise ratios relative to single stimulus presentations. These periodic responses, often called the _steady-state_ or _frequency following_ response, can be recorded from single neurons [@Enroth-Cugell1966] and local field potentials [@Morrone1987] using invasive methods, or with non-invasive electroencephalography (EEG) and magnetoencephalography (MEG) systems, both in humans [@Norcia2015] and in diverse animal species including insects [@Afsari2014], birds [@Porciatti1990], rodents [@Hwang2019] and primates [@Nakayama1982]. Steady-state methods are used to measure early sensory responses in vision [@Regan1966], hearing [@Rees1986] and somatosensation [@Snyder1992], and closely related paradigms have been developed to target specific stimulus features such as orientation [@Braddick1986], and facial expression [@Gray2020] and identity [@Liu-Shuang2014]. In fMRI research, _travelling wave_ methods [@Engel1994; @Sereno1995] are used to map the retinotopic responses of early visual cortex using stimuli that change periodically in spatial position. Finally, physiological reflexes such as the pupillary response to light can be entrained in a similar way [@Spitschan2014].

A convenient way to analyse the data from periodic stimulation experiments is to take the Fourier transform of the measured signal. The amplitude of the response at the stimulation frequency (and its harmonics - integer multiples of the stimulation frequency) is a convenient, and well-isolated index of the brain's response. Fourier spectra comprise both amplitude and phase information, which can be equivalently expressed as complex numbers with real and imaginary components. In many studies the phase information is routinely discarded, and statistical comparisons are performed on the amplitude data only. However an alternative is to use multivariate statistics, which take into account both the amplitude and phase information (represented as real and imaginary components). Multivariate methods have the advantage that they are more sensitive to weak signals, and therefore offer increased statistical power relative to univariate methods.

For pointwise and pairwise comparisons, Hotelling's $T^2$ statistic [@Hotelling1931] is a multivariate extension of the T-test. For the one-sample case, the test statistic is defined as:

\begin{equation}
\label{eq:t2eq}
T^2 = N(\bar{x} - \mu)' C^{-1} (\bar{x} - \mu),
\end{equation}

where _N_ is the number of observations, $\bar{x}$ is the multivariate sample mean, $\mu$ is the point of comparison, $C^{-1}$ is the inverse covariance matrix, and $'$ denotes vector transposition. Conceptually, the $T^2$ statistic extends the univariate T-statistic by incorporating the covariance between the dependent variables. Two-sample and paired variants are also available, and the test can be applied to an arbitrary number of dependent variables (though here we will consider only the bivariate case).

More recently, Victor and Mast [-@Victor1991] proposed a simpler version of $T^2$, called $T^2_{circ}$. The $T^2_{circ}$ statistic makes the strong assumption that the dependent variables (real and imaginary values) are uncorrelated and have equal variance. When these conditions are met, the test statistic is defined as:

\begin{equation}
\label{eq:t2c}
T^2_{circ} = (N-1)\frac{|\bar{x}-\mu|^2}{\Sigma|x_j - \bar{x}|^2}
\end{equation}

where $x_j$ denotes the $j$th observation of the dependent variables, and all other terms retain their previous meanings. Notice that no covariance term is present in equation \@ref(eq:t2c), because of the independence assumption. This makes the statistic simpler to calculate, but causes problems when the assumption is violated.

In the present paper, we develop best practice guidelines for performing statistical tests on multivariate Fourier components derived from periodic stimulation paradigms. We first demonstrate why parametric univariate statistics are inappropriate for such data, because amplitudes for weak signals are not normally distributed. We then investigate conditions under which we should use either the $T^2$ or $T^2_{circ}$ statistic. The range of sample sizes and effect sizes where $T^2_{circ}$ is more sensitive is identified. We develop a novel method for testing the assumptions of the $T^2_{circ}$ statistic, based on calculating the condition index of a multivariate data set. Appropriate methods for identifying outliers using the Mahalanobis distance are discussed, and a heuristic proposed. Next the logic of $T^2_{circ}$ is extended to situations with more than two levels of the independent variable, and the performance of this novel $ANOVA^2_{circ}$ statistic is compared to MANOVA. Finally, we demonstrate the proposed techniques on two example SSVEP data sets, and recommend some best practice guidelines for analysis decisions.

All scripts used to generate this manuscript are available at: [https://github.com/bakerdh/FourierStats](https://github.com/bakerdh/FourierStats). These include _R_ functions to implement one-sample Hotelling's [-@Hotelling1931] $T^2$ statistic, Victor & Mast's [-@Victor1991] $T^2_{circ}$ statistic, and the condition index and $ANOVA^2_{circ}$ statistics proposed in this paper.

# Fourier amplitudes violate parametric assumptions of univariate statistics

Many empirical studies use univariate T-tests or analysis of variance (ANOVA) to analyse periodic data. Specifically, the amplitude component of the Fourier spectrum at the stimulation frequency is used as the dependent variable, discarding the phase information. This is problematic, because the amplitude is an absolute quantity, and can never fall below zero. Distributions of amplitudes for weak signals are therefore positively skewed, and will generally violate the assumption of normality.

```{r amphists, fig.cap="Demonstration of skew in absolute Fourier amplitudes for weak signals. Signal strength is quantified as Cohen's d, defined as the ratio of the mean to the standard deviation of the sample. The upper row shows samples of 50 grey points, and the population mean (coloured points). The lower row shows kernel density functions generated from 100,000 amplitude values. Note that the mean phase of the signal is irrelevant for these simulations, and is shown in the positive x-direction for consistency.", fig.align="center", echo=FALSE, fig.width=12, fig.height=6}

npoints <- 50
dlist <- c(0,0.5,1,2,4)

plot(x=NULL,y=NULL,axes=FALSE,ann=FALSE, xlim=c(0,60), ylim=c(0,24))
for (n in 1:5){lines(c((n-1)*12,(n*12)-2),c(18,18),lwd=2)}
for (n in 1:5){lines(c(5,5)+(n-1)*12,c(13,23),lwd=2)}

for (n in 1:5){lines(c((n-1)*12,(n*12)-2),c(0,0),lwd=2)}
for (n in 1:5){lines(c(0,0)+(n-1)*12,c(0,10),lwd=2)}

text(0.5,19,'x',adj=0.5,cex=1.5)
text(4,13.5,'y',adj=0.5,cex=1.5)


for (d in 1:5){
simdata <- matrix(rnorm(npoints*2,mean=0,sd=1),nrow=npoints,ncol=2)
simdata[,1] <- simdata[,1] + dlist[d]
for (n in 1:npoints){lines(c(0,simdata[n,1])+5+(d-1)*12,c(0,simdata[n,2])+18,col=rgb(0,0,0,alpha=0.2))}
points(simdata[,1]+5+(d-1)*12,simdata[,2]+18,type='p',pch=16,col=rgb(0,0,0,alpha=0.2))
points(5+(d-1)*12+dlist[d],18,type='p',pch=16,cex=2,col=nbdtpal[d])
text(5+(d-1)*12,24,paste('d =',dlist[d]),adj=0.5,cex=1.5)
}


for (d in 1:5){
simdata <- matrix(rnorm(nsims*2,mean=0,sd=1),nrow=nsims,ncol=2)
simdata[,1] <- simdata[,1] + dlist[d]
absvals <- abs(complex(real=simdata[,1],imaginary=simdata[,2]))
a <- density(absvals)
a$x <- a$x-min(a$x)
a$x <- a$x/max(a$x)
a$x <- 10*a$x
a$y <- 10*a$y/max(a$y)
polygon(a$x+(d-1)*12,a$y,col=addalpha(nbdtpal[d],alpha=0.3),border=NA)

# lines(seq(0,10,0.01)+(d-1)*12,10*pnorm(seq(-5,5,0.01)),lwd=2)
# q <- quantile(scale(absvals),probs=seq(0,1,0.001))
# lines(5+q+(d-1)*12,seq(0,10,0.01),lwd=2,col=nbdtpal[d])

}

```

The upper row of Figure \@ref(fig:amphists) shows scatterplots of simulated Fourier components, expressed using real (x) and imaginary (y) components. The amplitudes are the lengths of the lines joining each grey point to the origin. The lower row in Figure \@ref(fig:amphists) shows distributions of amplitudes for the same set of signal strengths. These distributions only approach normality when the signal strength is more than twice the standard deviation (Cohen's d > 2; Cohen's d is the mean difference scaled by the standard deviation, see Cohen [-@Cohen1988]). One consequence of this is that T-tests will potentially have an inflated Type 1 error (false positive) rate for many signals encountered empirically.

Typical solutions, such as log-transforming the data, are unlikely to be equally applicable to all conditions. For example, if one wishes to compare a baseline where no stimulus was presented with a condition involving a strong signal, the former will be skewed and the latter normal. Applying a transform to both conditions is therefore problematic. Non-parametric statistics are a potential option, but these have generally lower statistical power than their parametric equivalents. Instead, the bivariate statistics discussed in the introduction avoid these issues and have greater statistical power, and should be used in preference to univariate methods.

# Conditions under which $T^2_{circ}$ is more sensitive than $T^2$

Using the $T^2$ or $T^2_{circ}$ statistic allows us to retain the phase information, and therefore have greater power than univariate T-tests, as well as avoiding problems caused by using absolute amplitude values. Victor & Mast [-@Victor1991] report simulations showing situations where $T^2_{circ}$ has greater power than $T^2$. This involved generating random data sets of different sample sizes, and different signal strengths, and comparing the number of such tests where each statistic was significant. Their simulations show the largest advantage for $T^2_{circ}$ for effect sizes around d = 1 (where the mean signal strength is equal to the standard deviation of the data). The advantage appeared to be stronger for smaller sample sizes.

Here we replicate and extend these simulations (see Figure \@ref(fig:powerfig)), and confirm that the regime where $T^2_{circ}$ has greater power occurs particularly for large effect sizes and small sample sizes (see Figure \@ref(fig:powerfig)f). However, for effect sizes around 0.5 < _d_ < 1, $T^2_{circ}$ is more sensitive even with around 16 observations. This advantage is lost for large sample sizes (N > 32) and large effect sizes (d > 2 when N > 8). These simulations suggest a straightforward heuristic - there is no advantage to using the $T^2_{circ}$ statistic for sample sizes, so its use should be restricted to small sample studies.

```{r powerfig, fig.cap="Simulations estimating the proportion of significant tests for simulated data with different sample sizes and effect sizes. Panels a-c replicate conditions reported by Victor and Mast (1991). Panels (d) and (e) show a wider range of conditions for each statistic. Panel (f) shows the difference between the two statistics, with contour lines indicating differences of 0.02, 0.05, 0.1, 0.2 and 0.4.", fig.align="center", echo=FALSE, fig.width=12, fig.height=8}

layout(matrix(1:6,nrow=2,ncol=3,byrow=TRUE))
abc <- c('a','b','c')

vinds <- c(2,5,17)
dlist <- seq(0,4,0.25)
slist <- seq(1.5,5,0.5)
samplesizes <- ceiling(2^slist)

tsqmatrix <- matrix(0,nrow=length(samplesizes),ncol=length(dlist))
circmatrix <- matrix(0,nrow=length(samplesizes),ncol=length(dlist))
for (n in 1:nsims){
  for (s in 1:length(samplesizes)){
    for (s2 in 1:length(dlist)){
    data <- matrix(rnorm(2*samplesizes[s]),nrow=samplesizes[s],ncol=2)
    data[,1] <- data[,1] + dlist[s2]
    a <- tsq1.test(data)
    if (a[5]<0.05){tsqmatrix[s,s2] <- tsqmatrix[s,s2] + 1}
    a <- tsqc.test(data)
    if (a[5]<0.05){circmatrix[s,s2] <- circmatrix[s,s2] + 1}
  }}
}

tsqpropM <- tsqmatrix/nsims
circpropM <- circmatrix/nsims
tsqvscirc <- circpropM - tsqpropM
tsqvscirc <- pmax(tsqvscirc,0)  # sets any negative values to 0

plotlims <- c(1,5,0,1)
ticklocsx <- seq(1,5,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,1,0.2)    # locations of tick marks on y axis
ticklabelsx <- 2^ticklocsx        # set labels for x ticks
ticklabelsy <- ticklocsy    # set labels for y ticks

for (n in 1:3){
par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
title(ylab="Proportion significant", col.lab=rgb(0,0,0), line=2.2, cex.lab=1.5)
title(paste('d =',dlist[vinds[n]]),cex.main=2)
lines(slist,tsqpropM[,vinds[n]],lwd=3,col=nbdtpal[1])
lines(slist,circpropM[,vinds[n]],lwd=3,col=nbdtpal[2])

points(slist,tsqpropM[,vinds[n]],lwd=3,pch=15,cex=1.5,col=nbdtpal[1])
points(slist,circpropM[,vinds[n]],lwd=3,pch=16,cex=1.5,col=nbdtpal[2])

text(1.1,0.95,paste('(',abc[n],')',sep=''),cex=2)

if (n==1){legend(3.8,1,c(expression(T^2),expression('T'[circ]^2)),pch=15:16,cex=1.5,col=nbdtpal[1:2],box.lwd=2)}
}


par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(1,5), ylim=c(0,4))
ticklocsx <- seq(1,5,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,4,1)    # locations of tick marks on y axis
ticklabelsx <- 2^ticklocsx        # set labels for x ticks
ticklabelsy <- seq(0,4,1)    # set labels for y ticks
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)  
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)     
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Effect size (d)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
title(expression(T^2),cex.main=2)
image(slist,dlist,tsqpropM,zlim=c(0,1),add=TRUE,col=kovesi.linear_blue_95_50_c20(256))
text(1.1,3.8,'(d)',cex=2)
legend(3.4,3.8,c('0','0.5','1'),lwd=6,cex=1.5,col=kovesi.linear_blue_95_50_c20(3),box.lwd=2,bg='white',title='Proportion')

par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(1,5), ylim=c(0,4))
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)  
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)     
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Effect size (d)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
ttext <- expression('T'[circ]^2)
title(ttext,cex.main=2)
image(slist,dlist,circpropM,zlim=c(0,1),add=TRUE,col=kovesi.linear_blue_95_50_c20(256))
text(1.1,3.8,'(e)',cex=2)


par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(1,5), ylim=c(0,4))
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)  
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)     
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Effect size (d)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
ttext <- expression({'T'[circ]^2}~-~{T^{"2"}})
title(ttext,cex.main=2)
image(slist,dlist,tsqvscirc,zlim=c(0,1),add=TRUE,col=kovesi.linear_blue_95_50_c20(256))
cl <- contourLines(slist,dlist,tsqvscirc,levels=c(0.02,0.05,0.1,0.2,0.4))
for (n in 1:length(cl)){
  temp <- cl[n]
  lines(temp[[1]]$x,temp[[1]]$y,col='black',lwd=3)
}
text(1.1,3.8,'(f)',cex=2)


```

# Limitations of $T^2_{circ}$ when assumptions are violated

Although $T^2_{circ}$ can be more sensitive than $T^2$, this greater sensitivity relies on meeting the test's more stringent assumptions. The two variables must be independent, and of equal variance. These restrictions may hold for some data sets, but it is instructive to ask what happens when they do not. Figure \@ref(fig:falsealarms) shows the results of simulations with randomly generated bivariate data in which no signal is present. When the data are uncorrelated and have equal variance (mid-points of the functions in each panel), both tests have the nominal Type I error (false positive) rate of $\alpha = 0.05$ (dashed line). However, as the data become increasingly correlated (Figure \@ref(fig:falsealarms)a), or the variances of the two dependent variables more disparate (Figure \@ref(fig:falsealarms)b), the Type I error rate of the $T^2_{circ}$ statistic (shown in red) increases by almost a factor of 2. In contrast, the $T^2$ statistic, which explicitly takes account of the covariance matrix (see equation \@ref(eq:t2eq)) shows no increase (black curves).

```{r falsealarms, fig.width=6, fig.height=9, fig.cap='Simulations showing the Type I error rate for both tests as a function of the correlation between two variables (a) and the ratio of variances (b). Estimates are for 100000 simulations per condition, with N=10 observations. The icons at the foot of each panel show example scatterplots with bounding ellipses and eigenvectors.', echo=FALSE}

par(mfrow=c(2,1))

nobs <- 10
corvals <- seq(-0.9,0.9,0.1)

plotlims <- c(-1,1,0,0.1) 
ticklocsx <- seq(-1,1,0.5)    # locations of tick marks on x axis
ticklocsy <- seq(0,0.1,0.02)    # locations of tick marks on y axis
ticklabelsx <- ticklocsx        # set labels for x ticks
ticklabelsy <- ticklocsy    # set labels for y ticks

plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx)     # add the tick labels
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Correlation (R)", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)  
title(ylab="Proportion significant", col.lab=rgb(0,0,0), line=2, cex.lab=1.5)

lines(c(-1,1),c(0.05,0.05),lty=2,lwd=2)

hsig <- corvals*0
csig <- corvals*0
simdata <- matrix(0,nrow=nobs,ncol=2)
for (cond in 1:length(corvals)){
  for (simno in 1:nsims){
    
    xdata <- rnorm(nobs,mean=0,sd=1)
    ydata <- sign(corvals[cond])*sqrt(abs(corvals[cond]))*xdata + sqrt(1-abs(corvals[cond]))*rnorm(nobs,mean=0,sd=1)
    simdata[,1] <- xdata
    simdata[,2] <- ydata
    output <- tsqc.test(simdata,paired=FALSE)
    if (output$p.value < 0.05){csig[cond] <- csig[cond] + 1}
    output <- tsq1.test(simdata)
    if (output$p.value < 0.05){hsig[cond] <- hsig[cond] + 1}
  }}
propc <- csig/nsims
lines(corvals,propc,lty=1,lwd=4,col=nbdtpal[2])
proph <- hsig/nsims
lines(corvals,proph,lty=1,lwd=4,col=nbdtpal[1])

legend(-0.25,0.1, c(expression('T'[circ]^2),expression('T'^2)), cex=1, col=nbdtpal[2:1],lty=1, lwd=3, box.lwd=2)

nobs <- 10000
corvals <- seq(-0.9,0.9,0.45)
simdata <- matrix(0,nrow=nobs,ncol=2)

for (cond in 1:length(corvals)){
  xdata <- rnorm(nobs,mean=0,sd=1)
  ydata <- sign(corvals[cond])*sqrt(abs(corvals[cond]))*xdata + sqrt(1-abs(corvals[cond]))*rnorm(nobs,mean=0,sd=1)
  compdata <- data.frame(xdata,ydata)
  
  A <- cov(compdata)
  ctr    <- colMeans(compdata) 
  RR     <- chol(A)                               # Cholesky decomposition
  angles <- seq(0, 2*pi, length.out=200)          # angles for ellipse
  ell    <- 1 * cbind(cos(angles), sin(angles)) %*% RR  # ellipse scaled with factor 1
  ellCtr <- sweep(ell, 2, ctr, "+")               # center ellipse to the data centroid
  eigVal  <- eigen(A)$values
  eigVec  <- eigen(A)$vectors
  eigScl  <- eigVec  %*% diag(sqrt(eigVal))  # scale eigenvectors to length = square-root
  xMat    <- rbind(ctr[1] + eigScl[1, ], ctr[1] - eigScl[1, ])
  yMat    <- rbind(ctr[2] + eigScl[2, ], ctr[2] - eigScl[2, ])
  ellBase <- cbind(sqrt(eigVal[1])*cos(angles), sqrt(eigVal[2])*sin(angles)) 
  ellRot  <- eigVec %*% t(ellBase) 
  
  points(0.05*xdata[1:100]+corvals[cond],0.005*ydata[1:100]+0.02,pch=16,cex=0.5,col=nbdtpal[3])
  matlines(0.1*xMat+corvals[cond], 0.01*yMat+0.02, lty=1, lwd=3, col="black")
  lines(0.1*(ellRot+ctr)[1, ]+corvals[cond], 0.01*(ellRot+ctr)[2, ]+0.02, lwd=2)
}
text(-0.95,0.095,'(a)',cex=1.5)


nobs <- 10
corvals <- seq(-0.9,0.9,0.1)
varratios <- 10^(corvals/2)

plotlims <- c(-1,1,0,0.1) 
ticklocsx <- seq(-1,1,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,0.1,0.02)    # locations of tick marks on y axis
ticklabelsx <- c('1/10','1','10')        # set labels for x ticks
ticklabelsy <- ticklocsy    # set labels for y ticks

plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx)     # add the tick labels
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Variance ratio (y/x)", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)  
title(ylab="Proportion significant", col.lab=rgb(0,0,0), line=2, cex.lab=1.5)

lines(c(-1,1),c(0.05,0.05),lty=2,lwd=2)

hsig <- corvals*0
csig <- corvals*0
simdata <- matrix(0,nrow=nobs,ncol=2)
for (cond in 1:length(corvals)){
  for (simno in 1:nsims){
    
    xdata <- rnorm(nobs,mean=0,sd=sqrt(1/varratios[cond]))
    ydata <- rnorm(nobs,mean=0,sd=sqrt(varratios[cond]))
    simdata[,1] <- xdata
    simdata[,2] <- ydata
    
    output <- tsqc.test(simdata,paired=FALSE)
    if (output$p.value < 0.05){csig[cond] <- csig[cond] + 1}
    output <- tsq1.test(simdata)
    if (output$p.value < 0.05){hsig[cond] <- hsig[cond] + 1}
  }}
propc <- csig/nsims
lines(corvals,propc,lty=1,lwd=4,col=nbdtpal[2])
proph <- hsig/nsims
lines(corvals,proph,lty=1,lwd=4,col=nbdtpal[1])

legend(-0.25,0.1, c(expression('T'[circ]^2),expression('T'^2)), cex=1, col=nbdtpal[2:1],lty=1, lwd=3, box.lwd=2)

nobs <- 10000
corvals <- seq(-0.9,0.9,0.45)
varratios <- 10^(corvals/2)
simdata <- matrix(0,nrow=nobs,ncol=2)

for (cond in 1:length(corvals)){
  xdata <- rnorm(nobs,mean=0,sd=sqrt(1/varratios[cond]))
  ydata <- rnorm(nobs,mean=0,sd=sqrt(varratios[cond]))
  compdata <- data.frame(xdata,ydata)
  
  A <- cov(compdata)
  ctr    <- colMeans(compdata) 
  RR     <- chol(A)                               # Cholesky decomposition
  angles <- seq(0, 2*pi, length.out=200)          # angles for ellipse
  ell    <- 1 * cbind(cos(angles), sin(angles)) %*% RR  # ellipse scaled with factor 1
  ellCtr <- sweep(ell, 2, ctr, "+")               # center ellipse to the data centroid
  eigVal  <- eigen(A)$values
  eigVec  <- eigen(A)$vectors
  eigScl  <- eigVec  %*% diag(sqrt(eigVal))  # scale eigenvectors to length = square-root
  xMat    <- rbind(ctr[1] + eigScl[1, ], ctr[1] - eigScl[1, ])
  yMat    <- rbind(ctr[2] + eigScl[2, ], ctr[2] - eigScl[2, ])
  ellBase <- cbind(sqrt(eigVal[1])*cos(angles), sqrt(eigVal[2])*sin(angles)) 
  ellRot  <- eigVec %*% t(ellBase) 
  
  points(0.05*xdata[1:100]+corvals[cond],0.005*ydata[1:100]+0.02,pch=16,cex=0.5,col=nbdtpal[3])
  matlines(0.1*xMat+corvals[cond], 0.01*yMat+0.02, lty=1, lwd=3, col="black")
  lines(0.1*(ellRot+ctr)[1, ]+corvals[cond], 0.01*(ellRot+ctr)[2, ]+0.02, lwd=2)
}
text(-0.95,0.095,'(b)',cex=1.5)

```

One possible remedy to control the Type I error rate would be to adjust the alpha level to compensate. However, this will reduce the statistical power of the $T^2_{circ}$ test, and its advantage over $T^2$ is relatively marginal in most situations to begin with (see Figure \@ref(fig:powerfig)). What is required is a method to objectively assess whether the assumptions of $T^2_{circ}$ hold, which we develop in the following section.

# A novel method to test the assumptions of $T^2_{circ}$

Despite the severe consequences of violating the assumptions of the $T^2_{circ}$ statistic (see Figure \@ref(fig:falsealarms)), there is currently no accepted test for those assumptions that could be applied to an empirical data set. Victor and Mast [-@Victor1991] suggest that their test should be applicable to multiple repetitions of a stimulus condition collected from a single participant, whereas data pooled across multiple participants may be less likely to exhibit independence of the real and imaginary components [see also @Pei2017]. 

One convenient way to test the assumptions of $T^2_{circ}$ is to assess the _condition index_ of a data set, which describes the ratio of eigenvalues for a cloud of points. The eigenvectors are the major and minor axes of the bounding ellipse (the straight lines in the example icons at the foot of Figure \@ref(fig:falsealarms)a,b). Conventionally, the condition index is calculated as the square root of the longest/shortest eigenvector length. For uncorrelated random numbers the distribution of condition indices is positively skewed, with a minimum of 1 [@Edelman1988]. This is because two independent samples of numbers from the same underlying distribution will still by chance have unequal eigenvectors. Edelman [-@Edelman1988] provides an equation (Eq 14) for the probability density function of condition indices (x) as a function of sample size (n):

\begin{equation}
\label{eq:edelman1}
pdf = (n-1)2^{n-1}\frac{x^2 - 1}{(x^2 + 1)^n}x^{(n-2)},
\end{equation}

Attempts to validate this by simulation suggest that for small sample sizes (n<10) a closer approximation is given by:

\begin{equation}
\label{eq:edelman2}
pdf = (n-2)2^{n-2}\frac{x^2 - 1}{(x^2 + 1)^{(n-1)}}x^{(n-3)}
\end{equation}

In Figure \@ref(fig:distcomparison)a we show an example distribution derived from the second expression (black curve), the inverse cumulative density function (red curve), and simulations from 100000 random data sets for n=10 (blue shading). The vertical lines show the critical (95%) threshold for the analytic and simulated results - a ratio lying beyond this threshold can be considered to violate the assumption of either independence or equal variance. Figure \@ref(fig:distcomparison)b shows how these thresholds change as a function of the number of observations, and it is clear that the modified expression (red) most closely approximates the simulation results (blue).

```{r distcomparison, fig.cap='Panel (a) shows the distribution of condition indices derived from Equation 4 (black curve), and by simulation (blue shading), for a sample size of 10 observations. The vertical blue and red lines show the 95 percent thresholds on the curve (where 95 percent of values lie to the left of the line), and the red curve shows the inverse density function. Panel (b) shows how 95 percent thresholds change as a functio of the number of observations.', fig.align="center", fig.width=9, fig.height=5, echo=FALSE}

par(mfrow=c(1,2), las=1)

n <- 10
x <- seq(1,20,0.01)
nr <- n-1
pdffunction <- ((nr-1)*(2^(nr-1))) * ((x^2 - 1)/((x^2+1)^nr)) * (x^(nr-2))

plotlims <- c(1,5,0,1) 
ticklocsx <- seq(1,5,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,1,0.2)    # locations of tick marks on y axis
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklocsx, side = 1, at=ticklocsx)     # add the tick labels
mtext(text = ticklocsy, side = 2, at=ticklocsy, line=0.2) 
title(xlab="Condition index", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)  
title(ylab="Probability", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)

lines(x,pdffunction/max(pdffunction),lwd=3)
cdfinverse <- 1-(cumsum(pdffunction)/sum(pdffunction))
lines(x,cdfinverse,col=nbdtpal[2],lwd=3)
lines(c(1,5),c(0.05,0.05),lty=2,col=nbdtpal[1])
criticalx <- x[min(which(cdfinverse<=0.05))]
lines(c(criticalx,criticalx),c(0,1),col=nbdtpal[2],lwd=3)

simvect <- NULL
for (s in 1:nsims){
  simdata <- matrix(rnorm(2*n,mean=0,sd=1),nrow=n,ncol=2)
  evs <- eigen(cov(simdata)) 
  simvect[s] <- sqrt(evs$values[1]/evs$values[2])
}

a <- hist(simvect,breaks = 200, plot=FALSE)
axvals <- a$mids
ayvals <- a$counts/max(a$counts)
polygon(c(1,axvals,1), c(0,ayvals,0), col=addalpha(nbdtpal[3],alpha=0.2),border=NA)
simcrit <- quantile(simvect,0.95)
lines(c(simcrit,simcrit),c(0,1),col=nbdtpal[3],lwd=3,lty=2)
text(4.8,0.95,'(a)',cex=1.5)


x <- seq(1,50,0.01)
edelman1 <- NULL
edelman2 <- NULL
simratios <- NULL
for (n in 3:25){
nr <- n
exactpdf <- ((nr-1)*(2^(nr-1))) * ((x^2 - 1)/((x^2+1)^nr)) * (x^(nr-2))
cdfinverse <- 1-(cumsum(exactpdf)/sum(exactpdf))
edelman1[n-2] <- x[min(which(cdfinverse<=0.05))]

nr <- n - 1
exactpdf <- ((nr-1)*(2^(nr-1))) * ((x^2 - 1)/((x^2+1)^nr)) * (x^(nr-2))
cdfinverse <- 1-(cumsum(exactpdf)/sum(exactpdf))
edelman2[n-2] <- x[min(which(cdfinverse<=0.05))]

simvect <- NULL
for (s in 1:nsims){
  simdata <- matrix(rnorm(2*n,mean=0,sd=1),nrow=n,ncol=2)
  evs <- eigen(cov(simdata))
  simvect[s] <- sqrt(evs$values[1]/evs$values[2])
}
simratios[n-2] <- quantile(simvect,0.95)
}

plotlims <- c(0,25,0,25) 
ticklocsx <- seq(0,25,5)    # locations of tick marks on x axis
ticklocsy <- seq(0,25,5)    # locations of tick marks on y axis
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklocsx, side = 1, at=ticklocsx)     # add the tick labels
mtext(text = ticklocsy, side = 2, at=ticklocsy, line=0.2) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Threshold", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)

lines(3:25,edelman1,lwd=3,col=nbdtpal[1])
lines(3:25,edelman2,lwd=3,col=nbdtpal[2])
lines(3:25,simratios,lwd=3,col=nbdtpal[3])
legend(12, 15, c("Edelman","Modified","Simulation"), cex=0.8, col=nbdtpal, lty=1, lwd=3, box.lwd=2)
text(23,24,'(b)',cex=1.5)

```

The eigenvalue ratio can be used as a test of the assumptions of $T^2_{circ}$. If we observe a condition index above the critical threshold for the number of observations then the data set can be said to significantly violate the assumption of equal eigenvalues. Because our modified equation provides an inverse density function (red curve in Figure \@ref(fig:distcomparison)a), we can use this to calculate a p-value for the test. If the test is non-significant, one can proceed with $T^2_{circ}$; if it is significant, $T^2$ should be used instead. An _R_ function implementing this test is included in the project code repository (the _CI.test_ function contained in the _CItest.R_ script).

# Identifying and removing outliers using the Mahalanobis distance

If a data set produces a significant result using the condition index test, this could be due to the presence of one or more outliers. The Mahalanobis distance [@Mahalanobis1936] is a useful metric for identifying such multivariate outliers so that they can be excluded. It calculates the Euclidean distance between each data point and the centroid, and scales it by the variance in the direction of the vector that joins the two points. This means that any correlations in the data set are taken into account when calculating the distance metric, D.

The effectiveness of this approach to outlier exclusion can be assessed by simulation using the condition index test. Figure \@ref(fig:outlierplot) shows the proportion of significant condition index tests as a function of the Mahalanobis distance of a single outlier, for a range of sample sizes (curves). In all cases, the functions depart from the Type 1 error rate ($\alpha$ = 0.05; horizontal dashed line in Figure \@ref(fig:outlierplot)) when the Mahalanobis distance exceeds a value around 3. This seems a reasonable heuristic for outlier exclusion, and is the multivariate equivalent of excluding data points more than 3 standard deviations from the mean (note that many implementations of the Mahalanobis distance statistic return $D^2$, which can be converted to D by taking the square root). Following this heuristic should reduce the likelihood that outliers will invalidate the assumptions of the $T^2_{circ}$ test.

```{r outlierplot, fig.cap='Simulations illustrating the Mahalanobis distance metric, and showing how a single outlier affects the condition index. The upper row shows three example data sets, each with a single outlier shown in red. The outliers have Mahalanobis distances of 1, 3 and 5. The ellipses are calculated with the outlier included (red) and excluded (black), illustrating how the outlier distorts the aspect ratio of the ellipse. The lower plot shows how the proportion of significant condition index tests depends on the outlier distance and the sample size.', fig.align="center", fig.width=7, fig.height=6, echo=FALSE}

set.seed(2112)
nobs <- 16
mdists <- c(1,3,5)
xlocs <- c(1,3.5,6)
  
plotlims <- c(0,8,0,2) 
ticklocsx <- seq(0,8,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,1,0.2)    # locations of tick marks on y axis
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklocsx, side = 1, at=ticklocsx)     # add the tick labels
mtext(text = ticklocsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Outlier Mahalanobis distance (D)", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
mtext(text = "Proportion significant", side = 2, at=0.5, line=1.5, cex=1.5)
points(0,2,pch=15,col=rgb(0.99,0.99,0.99),cex=0.1)


  xdata <- rnorm(nobs,mean=0,sd=1)
  ydata <- rnorm(nobs,mean=0,sd=1)
  compdata <- data.frame(xdata,ydata)
  ctr <- colMeans(compdata)
  ellRot <- getel(compdata)
  
  for (m in 1:3){
  xdata[1] <- mdists[m]
  ydata[1] <- 0
  compdata <- data.frame(xdata,ydata)
  ctr2 <- colMeans(compdata)
  ellRot2 <- getel(compdata)

  polygon(0.4*(ellRot2+ctr2)[1, ]+xlocs[m], 0.1*(ellRot2+ctr2)[2, ]+1.5, col=addalpha(nbdtpal[2],0.3),border=NA)
  lines(0.4*(ellRot+ctr)[1, ]+xlocs[m], 0.1*(ellRot+ctr)[2, ]+1.5, lwd=1)
  lines(0.4*c(ctr2[1],xdata[1])+xlocs[m],0.1*c(ctr2[2],ydata[1])+1.5,col=nbdtpal[2],lwd=3)
  points(0.4*xdata+xlocs[m],0.1*ydata+1.5,pch=16,cex=0.5,col=nbdtpal[3])
  points(0.4*ctr2[1]+xlocs[m],0.1*ctr2[2]+1.5,pch=16,cex=1,col=nbdtpal[1])
  points(0.4*xdata[1]+xlocs[m],0.1*ydata[1]+1.5,pch=16,cex=1,col=nbdtpal[2])
}

  text(1,1.8,'D = 1',cex=1.5,adj=0.5)
  text(3.5,1.8,'D = 3',cex=1.5,adj=0.5)
  text(6,1.8,'D = 5',cex=1.5,adj=0.5)

dlist <- seq(0,8,0.5)
slist <- seq(2,6,1)
samplesizes <- ceiling(2^slist)

nsig <- matrix(0,nrow=length(samplesizes),ncol=length(dlist))
for (s in 1:length(samplesizes)){
  for (d in 1:length(dlist)){
    for (n in 1:nsims){
    data <- matrix(rnorm(2*samplesizes[s]),nrow=samplesizes[s],ncol=2)
    data[1,] <- c(dlist[d],0)
    output <- CI.test(data)
    if(output$pval<0.05){nsig[s,d] <- nsig[s,d] + 1}
    }
  }
}

propsig <- nsig/nsims

lines(c(0,8),c(0.05,0.05),lty=2,lwd=2)

for (s in 1:length(samplesizes)){
  lines(dlist,propsig[s,],lwd=3,col=nbdtpal[s])
}

legend(0,1,samplesizes,col=nbdtpal,lwd=3,box.lwd=2,title='N observations')

```

A variant of the Mahalanobis distance (the pairwise Mahalanobis distance) can also be used to compute a multivariate measure of effect size, equivalent to Cohen's _d_ statistic [see e.g. @Giudice2009]. This is a valuable statistic to include when reporting the results of multivariate tests, and we include an _R_ function (_mahalES_) that can be used to calculate it.

# Generalising to more than two conditions

Many studies involve more than two conditions that need to be compared. Issues with familywise error will quickly become problematic if multiple pairwise $T^2$ or $T^2_{circ}$ statistics are calculated. One possibility is to conduct a MANOVA, which takes covariances between dependent variables into account in much the same way as Hotelling's $T^2$, but permits multiple independent variables with more than two levels. However, if the assumptions of $T^2_{circ}$ hold for a data set, it might alternatively be possible to extend to logic of the $T^2_{circ}$ test [@Victor1991] to the more general case. 

The F statistic for a one-way ANOVA is calculated by taking the ratio of the variance explained by the modelled group means to the residual unexplained variance. For multivariate data, the change in group mean would be calculated using the vector distances between the complex means, and the residuals are the vector distances between each data point and its corresponding group mean. For an independent one-way design with $k$ groups (or conditions) and $N$ observations per group, the F distribution will have $2(k-1)$ and $2((Nk)-k)$ degrees of freedom. The difference between traditional univariate ANOVA is the factor of two scaling, which accounts for the additional degree of freedom for each complex-valued number. A suitable name for such a test might be ${ANOVA}^2_{circ}$, as this reflects the similarity to ANOVA, and the extension of the logic of $T^2_{circ}$ (an alternative name might be $MANOVA_{circ}$, however this feels less appropriate given that many of the key features of MANOVA are absent).

Figure \@ref(fig:powerfig2) shows simulations analogous to those in Figure \@ref(fig:powerfig) for a one-way design with three levels. MANOVA is directly compared to the ${ANOVA}^2_{circ}$ statistic across a range of effect sizes and sample sizes. Just as for the one-sample statistics, the advantages of ${ANOVA}^2_{circ}$ are particularly apparent for small sample sizes, and larger effect sizes (Figure \@ref(fig:powerfig2)f).

```{r powerfig2, fig.cap="Simulations comparing the sensitivity of MANOVA and ANOVA2circ. The format mirrors that of Figure 2. In these simulations, there were three conditions, with the signal being added to one condition.", fig.align="center", echo=FALSE, fig.width=12, fig.height=8}

layout(matrix(1:6,nrow=2,ncol=3,byrow=TRUE))
abc <- c('a','b','c')

vinds <- c(2,5,17)
dlist <- seq(0,4,0.25)
slist <- seq(1.5,5,0.5)
samplesizes <- ceiling(2^slist)

manovamatrix <- matrix(0,nrow=length(samplesizes),ncol=length(dlist))
circmatrix <- matrix(0,nrow=length(samplesizes),ncol=length(dlist))
for (n in 1:nsims){
  for (s in 1:length(samplesizes)){
    grouplabels <- as.factor(rep(1:3,each=samplesizes[s]))

    for (s2 in 1:length(dlist)){
      
  data <- matrix(rnorm(3*2*samplesizes[s]),nrow=3*samplesizes[s],ncol=2)
  data[1:samplesizes[s],1] <- data[1:samplesizes[s],1] + dlist[s2]
  
  simdata <- complex(real=data[,1],imaginary=data[,2])
  
  output <- anovacirc.test(data.frame(simdata,grouplabels))
  if (output$pvalue<0.05){circmatrix[s,s2] <- circmatrix[s,s2] + 1}

dataforManova <- data.frame(grouplabels,data[,1],data[,2])    
colnames(dataforManova) <- c("Group","Real","Imaginary")
outcome <- cbind(dataforManova$Real, dataforManova$Imaginary)
manout <- summary(manova(outcome ~ Group, data = dataforManova))
  if (manout$stats[1,6]<0.05){manovamatrix[s,s2] <- manovamatrix[s,s2] + 1}

  }}
}

manovapropM <- manovamatrix/nsims
circpropM <- circmatrix/nsims
manvscirc <- circpropM - manovapropM
manvscirc <- pmax(manvscirc,0)  # sets any negative values to 0

plotlims <- c(1,5,0,1)
ticklocsx <- seq(1,5,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,1,0.2)    # locations of tick marks on y axis
ticklabelsx <- 2^ticklocsx        # set labels for x ticks
ticklabelsy <- ticklocsy    # set labels for y ticks

for (n in 1:3){
par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
title(ylab="Proportion significant", col.lab=rgb(0,0,0), line=2.2, cex.lab=1.5)
title(paste('d =',dlist[vinds[n]]),cex.main=2)
lines(slist,manovapropM[,vinds[n]],lwd=3,col=nbdtpal[1])
lines(slist,circpropM[,vinds[n]],lwd=3,col=nbdtpal[2])

points(slist,manovapropM[,vinds[n]],lwd=3,pch=15,cex=1.5,col=nbdtpal[1])
points(slist,circpropM[,vinds[n]],lwd=3,pch=16,cex=1.5,col=nbdtpal[2])

text(1.1,0.95,paste('(',abc[n],')',sep=''),cex=2)

if (n==1){legend(3,1,c('MANOVA',expression('ANOVA'[circ]^2)),pch=15:16,cex=1.5,col=nbdtpal[1:2],box.lwd=2)}
}


par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(1,5), ylim=c(0,4))
ticklocsx <- seq(1,5,1)    # locations of tick marks on x axis
ticklocsy <- seq(0,4,1)    # locations of tick marks on y axis
ticklabelsx <- 2^ticklocsx        # set labels for x ticks
ticklabelsy <- seq(0,4,1)    # set labels for y ticks
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)  
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)     
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Effect size (d)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
title(expression('MANOVA'),cex.main=2)
image(slist,dlist,manovapropM,zlim=c(0,1),add=TRUE,col=kovesi.linear_blue_95_50_c20(256))
text(1.1,3.8,'(d)',cex=2)
legend(3.4,3.8,c('0','0.5','1'),lwd=6,cex=1.5,col=kovesi.linear_blue_95_50_c20(3),box.lwd=2,bg='white',title='Proportion')

par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(1,5), ylim=c(0,4))
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)  
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)     
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Effect size (d)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
ttext <- expression('ANOVA'[circ]^2)
title(ttext,cex.main=2)
image(slist,dlist,circpropM,zlim=c(0,1),add=TRUE,col=kovesi.linear_blue_95_50_c20(256))
text(1.1,3.8,'(e)',cex=2)


par(pty="s")
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=c(1,5), ylim=c(0,4))
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)  
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx, line=0.2)     
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1) 
title(xlab="Number of observations", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)
title(ylab="Effect size (d)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
ttext <- expression({'ANOVA'[circ]^2}~-~{MANOVA})
title(ttext,cex.main=2)
image(slist,dlist,manvscirc,zlim=c(0,1),add=TRUE,col=kovesi.linear_blue_95_50_c20(256))
cl <- contourLines(slist,dlist,manvscirc,levels=c(0.02,0.05,0.1,0.2,0.4))
for (n in 1:length(cl)){
  temp <- cl[n]
  lines(temp[[1]]$x,temp[[1]]$y,col='black',lwd=3)
}
text(1.1,3.8,'(f)',cex=2)


```

Following ${ANOVA}^2_{circ}$, one could calculate $T^2_{circ}$ statistics to make post-hoc pairwise comparisons between conditions, providing that standard Bonferroni corrections are applied. Univariate ANOVAs on the real and imaginary components are unlikely to be informative, as the relative magnitudes depend on stimulus phase (which is arbitrary). A repeated measures version of ${ANOVA}^2_{circ}$ has also been implemented following the same logic. The project repository contains _R_ scripts for both of these functions.

# Deciding which test to run

The flowchart in Figure \@ref(fig:flowchart) contains a proposed decision structure for the analysis of periodic data, once any outliers have been removed. Initially, data for each condition should be tested against the expected distribution of eigenvalue ratios. Comparisons with one or two conditions should be tested with the $T^2_{circ}$ statistic if the eigenvalue ratios are consistent with circularity, and the $T^2$ statistic if not. Comparisons with more than two conditions should be tested with the $ANOVA^2_{circ}$ statistic if the eigenvalue ratios are consistent with circularity, or a MANOVA if not. Some MANOVA implementations cannot deal with random factors (repeated measures) in complex factorial designs. In _R_, the _Manova_ function from the _car_ package appears to be capable of the necessary calculations, as does the commercial software package SPSS.

```{r flowchart, fig.cap = 'Flowchart illustrating how one might decide which test to conduct for a given data set, based on the study design and the outcome of the condition index test.', fig.align="center", echo=FALSE}

pos <- coordinates(c(1,2,4))
plot(x=NULL,y=NULL,axes=FALSE,ann=FALSE, xlim=c(0,1), ylim=c(0,1))

mid <- straightarrow(from = pos[1, ], to = pos[2, ], lty = 1)
text(mid[1]-0.05, mid[2]+0.05, "N <= 2", cex = 0.8)
mid <- straightarrow(from = pos[1, ], to = pos[3, ], lty = 1)
text(mid[1]+0.05, mid[2]+0.05, "N > 2", cex = 0.8)
mid <- straightarrow(from = pos[2, ], to = pos[4, ], lty = 1)
text(mid[1]-0.05, mid[2]+0.01, "Yes", cex = 0.8)
mid <- straightarrow(from = pos[2, ], to = pos[5, ], lty = 1)
text(mid[1]+0.05, mid[2]+0.01, "No", cex = 0.8)
mid <- straightarrow(from = pos[3, ], to = pos[6, ], lty = 1)
text(mid[1]-0.05, mid[2]+0.01, "Yes", cex = 0.8)
mid <- straightarrow(from = pos[3, ], to = pos[7, ], lty = 1)
text(mid[1]+0.05, mid[2]+0.01, "No", cex = 0.8)

textellipse(mid = pos[1,], radx = 0.1, rady = 0.1, lab = 'How many levels?', cex = 0.5, shadow.col = "grey")
textellipse(mid = pos[2,], radx = 0.1, rady = 0.1, lab = 'Eigenvalues equal?', cex = 0.5, shadow.col = "grey")
textellipse(mid = pos[3,], radx = 0.1, rady = 0.1, lab = 'Eigenvalues equal?', cex = 0.5, shadow.col = "grey")
textellipse(mid = pos[4,], radx = 0.1, rady = 0.1, lab = expression(T[circ]^2), cex = 0.5, shadow.col = "grey")
textellipse(mid = pos[5,], radx = 0.1, rady = 0.1, lab = expression("Hotelling's T"^2), cex = 0.5, shadow.col = "grey")
textellipse(mid = pos[6,], radx = 0.1, rady = 0.1, lab = expression(ANOVA[circ]^2), cex = 0.5, shadow.col = "grey")
textellipse(mid = pos[7,], radx = 0.1, rady = 0.1, lab = 'MANOVA', cex = 0.5, shadow.col = "grey")

```

# Applying these methods to empirical data sets

Having developed some novel tools for the analysis of periodic data, in the following sections we demonstrate them using two different publicly available empirical data sets. The first study recorded responses to auditory and optogenetic stimulation in mice. The second study measured visual responses to flickering grating patterns in humans. These cases provide examples of how the results of the tests described above might be appropriately reported.

## Mouse auditory and optogenetic steady-state data

Hwang et al. [-@Hwang2019; -@Hwang2020] measured steady-state responses using implanted scalp electrodes in 6 mice. The mice had previously been given a targeted virus that made parvalbumin neurons in their basal forebrain responsive to specific wavelengths of light, delivered through an optical fiber (a technique called optogenetics). SSVEPs were recorded from 36 electrodes for 1 second epochs of 40 Hz auditory stimulation, and various schedules of optogenetic stimulation (including at 40 Hz). The data set is described more fully by Hwang et al. [-@Hwang2020], and was downloaded from: [https://doi.gin.g-node.org/10.12751/g-node.e5tyek/](https://doi.gin.g-node.org/10.12751/g-node.e5tyek/). 

Figure \@ref(fig:mousedata)a shows Fourier amplitude spectra at two frontal electrodes (marked black in the insets) for auditory stimulation (red) and optogenetic stimulation (blue), each at 40 Hz. There is a clear frequency-locked signal with approximately equal amplitude for each stimulation modality. Indeed, a paired univariate T-test on the amplitudes reveals no significant difference (_t_ = 0.84, _df_ = 5, _p_ = 0.44). However, inspection of the complex Fourier components for each condition suggests evidence of a phase difference between the two modalities (see Figure \@ref(fig:mousedata)b). The condition index test was non-significant for both conditions (sound: CI = 1.59, _p_ = 0.66; light: CI = 1.69, _p_ = 0.59), so a paired-samples $T^2_{circ}$ test was conducted. This revealed a significant difference between conditions ($T^2_{circ}$ = 1.39, $F_{(2,10)}$ = 8.32, _p_ = 0.007) with an effect size of D = 1.02. This demonstrates that both sound and (optogenetic) light are able to entrain neural responses - the original study by Hwang et al. [-@Hwang2019] went on to explore interactions between these two signals.

```{r mousedata, fig.cap = 'Summary of mouse steady-state responses to 40 Hz stimulation. Panel (a) shows the Fourier amplitude spectrum with inset scalp plots for sound (red) and light (blue) stimulation, averaged across repetitions and individuals. Panel (b) shows complex (x = real, y = imaginary) Fourier components for 6 individual mice (small points) and their average (large points) for both conditions.', fig.width=10, fig.height=6, fig.align="center", echo=FALSE}

load('Hwangdata.RData')

frequencies <- 1:100
f1 <- which(frequencies==40)

meandata <- abs(apply(mousedata,c(2,3,4),mean))

temp <- rowMeans(mousedata[,1:2,11,f1])
xdata <- Re(temp)
ydata <- Im(temp)
temp2 <- rowMeans(mousedata[,1:2,12,f1])
xdata2 <- Re(temp2)
ydata2 <- Im(temp2)

plot(x=NULL,y=NULL,axes=FALSE,ann=FALSE, xlim=c(0,150), ylim=c(0,2))
ticklocsx <- seq(0,80,20)    # locations of tick marks on x axis
ticklocsy <- seq(0,2,0.5)    # locations of tick marks on y axis
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)     # plot tick marks (no labels)
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklocsx, side = 1, at=ticklocsx)     # add the tick labels
mtext(text = ticklocsy, side = 2, at=ticklocsy, line=0.2, las=1)  # the 'line' command moves away from the axis, the 'las' command rotates to vertical
title(ylab="Amplitude (µV)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)
mtext('Frequency (Hz)',side=1,at=40,line=1.2,cex=1.5)

rmax <- 7   #specify a maximum boundary for the grid
gridRes <- 100 #specify the interpolation grid resolution
ramp <- colorRamp(c("white", rgb(187/255,40/255,107/255)))  # create a ramp from one colour to another
colmatrix2 <- rgb(ramp(seq(0, 1, length = 100)), max = 255)   # index the ramp at ten points

datatoplot <- abs(meandata[,11,f1])
testDat<- data.frame(x = montage[,2],
                     y = -montage[,3],
                     z = datatoplot)

xo <- seq(min(-rmax, testDat$x), max(rmax, testDat$x), length = gridRes)
yo <- seq(max(rmax, testDat$y), min(-rmax, testDat$y), length = gridRes)

interpV4 <- v4Interp(testDat, xo, yo, rmax, gridRes)

zo2 <- as.matrix(interpV4[,2:ncol(interpV4)])
xo2 <- matrix(rep(xo,length(yo)),nrow = length(xo),ncol = length(yo))
yo2 <- t(matrix(rep(yo,length(xo)),nrow = length(yo),ncol = length(xo)))
zo2[which(zo2>2)] <- 2

image(2.4*xo+20,1.5+xo/14,zo2,zlim=c(0,2),col=colmatrix2,add=TRUE,useRaster=TRUE)

rmax2 <- rmax+0.1
maskx <- outline[which(outline[,1]>=0),1]
masky <- outline[which(outline[,1]>=0),2]
polygon(2.4*c(0,maskx,0,rmax2,rmax2,0)+20,1.5+c(rmax2,masky,-rmax2,-rmax2,rmax2,rmax2)/14,border=NA,col="white")
maskx <- outline[which(outline[,1]<=0),1]
masky <- outline[which(outline[,1]<=0),2]
i <- sort(masky,index.return=TRUE)
maskx <- maskx[i$ix]
masky <- masky[i$ix]
polygon(2.4*c(0,maskx,0,-rmax2,-rmax2,0)+20,1.5+c(-rmax2,masky,rmax2,rmax2,-rmax2,-rmax2)/14,border=NA,col="white")

lines(2.4*outline[,1]+20,1.5+outline[,2]/14,lwd=2)
points(2.4*montage[3:38,2]+20,1.5+montage[3:38,3]/14,pch=16,col='grey')
points(2.4*montage[1:2,2]+20,1.5+montage[1:2,3]/14,pch=16)


ramp <- colorRamp(c("white", rgb(56/255,111/255,164/255)))  # create a ramp from one colour to another
colmatrix2 <- rgb(ramp(seq(0, 1, length = 100)), max = 255)   # index the ramp at ten points

datatoplot <- abs(meandata[,12,f1])
testDat<- data.frame(x = montage[,2],
                     y = -montage[,3],
                     z = datatoplot)

xo <- seq(min(-rmax, testDat$x), max(rmax, testDat$x), length = gridRes)
yo <- seq(max(rmax, testDat$y), min(-rmax, testDat$y), length = gridRes)

interpV4 <- v4Interp(testDat, xo, yo, rmax, gridRes)

zo2 <- as.matrix(interpV4[,2:ncol(interpV4)])
xo2 <- matrix(rep(xo,length(yo)),nrow = length(xo),ncol = length(yo))
yo2 <- t(matrix(rep(yo,length(xo)),nrow = length(yo),ncol = length(xo)))
zo2[which(zo2>2)] <- 2

image(2.4*xo+60,1.5+xo/14,zo2,zlim=c(0,2),col=colmatrix2,add=TRUE,useRaster=TRUE)

rmax2 <- rmax+0.1
maskx <- outline[which(outline[,1]>=0),1]
masky <- outline[which(outline[,1]>=0),2]
polygon(2.4*c(0,maskx,0,rmax2,rmax2,0)+60,1.5+c(rmax2,masky,-rmax2,-rmax2,rmax2,rmax2)/14,border=NA,col="white")
maskx <- outline[which(outline[,1]<=0),1]
masky <- outline[which(outline[,1]<=0),2]
i <- sort(masky,index.return=TRUE)
maskx <- maskx[i$ix]
masky <- masky[i$ix]
polygon(2.4*c(0,maskx,0,-rmax2,-rmax2,0)+60,1.5+c(-rmax2,masky,rmax2,rmax2,-rmax2,-rmax2)/14,border=NA,col="white")

lines(2.4*outline[,1]+60,1.5+outline[,2]/14,lwd=2)
points(2.4*montage[3:38,2]+60,1.5+montage[3:38,3]/14,pch=16,col='grey')
points(2.4*montage[1:2,2]+60,1.5+montage[1:2,3]/14,pch=16)

lines(frequencies[1:79],meandata[1,11,1:79],col=nbdtpal[2],lwd=3)
lines(frequencies[1:79],meandata[1,12,1:79],col=nbdtpal[3],lwd=3)
lines(c(90,150),c(1,1),lwd=2)
lines(c(120,120),c(0,2),lwd=2)

ellRot1 <- getel(data.frame(xdata,ydata))
ellRot2 <- getel(data.frame(xdata2,ydata2))
polygon(10*(ellRot1[1, ]+mean(xdata))+120, 1+(ellRot1[2, ]+mean(ydata))/3, col=addalpha(nbdtpal[2],0.3),border=NA)
polygon(10*(ellRot2[1, ]+mean(xdata2))+120, 1+(ellRot2[2, ]+mean(ydata2))/3, col=addalpha(nbdtpal[3],0.3),border=NA)

points(10*xdata+120,1+ydata/3,pch=16,col=nbdtpal[2])
points(10*xdata2+120,1+ydata2/3,pch=16,col=nbdtpal[3])
points(10*mean(xdata)+120,1+mean(ydata)/3,pch=21,bg=nbdtpal[2],cex=1.5)
points(10*mean(xdata2)+120,1+mean(ydata2)/3,pch=21,bg=nbdtpal[3],cex=1.5)

legend(130,0.6,c('Sound','Light'),pch=21,pt.cex=1.5,pt.bg=nbdtpal[2:3],box.lwd=2)

text(20,1.93,'Sound',cex=1.5,adj=0.5)
text(60,1.93,'Light',cex=1.5,adj=0.5)

text(0,1.95,'(a)',cex=2,adj=0.5)
text(90,1.95,'(b)',cex=2,adj=0.5)

text(149,0.94,'3',cex=1.5,adj=0.5)
text(122,1.97,'3',cex=1.5,adj=0.5)

# totest <- data.frame(xdata,ydata)
# totest2 <- data.frame(xdata2,ydata2)
# CI.test(totest)
# CI.test(totest2)
# tsqc.test(totest,y=totest2,paired=TRUE)
# 
# abs1 <- abs(rowMeans(mousedata[,1:2,11,f1]))
# abs2 <- abs(rowMeans(mousedata[,1:2,12,f1]))
# t.test(abs1,abs2,paired=TRUE)

# colnames(totest2) <- colnames(totest)
# fores <- rbind(totest,totest2)
# mahala_sq <- pairwise.mahalanobis(fores, rep(1:2,each=6))
# D <- sqrt(mahala_sq$distance[1,2])

```

## Human visual steady-state data

Vilidaite et al. [-@Vilidaite2018] measured visual responses to flickering grating stimuli in a large sample of 100 adults. Each participant completed a series of 11-second trials, in which stimuli of different contrasts flickered at 7Hz (on-off sinusoidal flicker). Responses were strongest at occipital electrodes over visual cortex (see upper row of Figure \@ref(fig:humanSSVEP)), were well-isolated in the Fourier domain, and increased with stimulus contrast. For this analysis, we took responses from electrode _Oz_ at the occipital pole (black points in the upper row of Figure \@ref(fig:humanSSVEP)), averaged across repetition for each participant. Each condition included some outlier points with Mahalanobis distances exceeding 3, marked red in the lower row of Figure \@ref(fig:humanSSVEP). Any participant that contributed at least one outlier was excluded, leaving a total of 89 participants for the main analysis.

```{r humanSSVEP, fig.cap = 'Summary of human SSVEP data. Upper row shows scalp distributions of Fourier amplitudes at 7Hz for stimuli of increasing contrasts (blue shading indicates higher amplitudes). Lower row shows scatterplots of complex (x = real, y = imaginary) Fourier components for 100 participants per condition, from electrode Oz (black point). Red points are outliers with Mahalanobis distances exceeding 3, and blue points mark the centroids.', fig.width=14, fig.height=5.4, fig.align="center", echo=FALSE}

load("/Users/danbaker/Google Drive/Current work/Research/powercontours/headplotspec.RData")

load('~/Google Drive/Current work/R scripts/danlabtoolbox/data/montagedata.RData')

subjmeans <- apply(alltarget[,31,,],1:2,mean)

rmax <- 0.55   #specify a maximum boundary for the grid
gridRes <- 100 #specify the interpolation grid resolution
# colmatrix2 <- kovesi.linear_kry_5_95_c72(101)
ramp <- colorRamp(c("white", rgb(56/255,111/255,164/255)))
colmatrix2 <- rgb(ramp(seq(0, 1, length = 100)), max = 255)   # index the ramp at ten 

selectrodes <- 'Oz'
montagelabels <- unlist(montage$labels)
blackelectrodes <- match(toupper(selectrodes),montagelabels,nomatch=0)

plot(x=NULL,y=NULL,axes=FALSE,ann=FALSE, xlim=c(0,8), ylim=c(0,2))
nexcluded <- NULL
isincluded <- 1 + 0*(1:100)

for (cond in 1:7){
datatoplot <- abs(apply(alltarget[,1:64,cond,], 2, mean))
datatoplot[c(13,19)] <- mean(datatoplot)   # set the mastoids to 0
testDat<- data.frame(x = montage$electrodelocs[montage$channelmappings,1],
                     y = -montage$electrodelocs[montage$channelmappings,2],
                     z = datatoplot)

xo <- seq(min(-rmax, testDat$x), max(rmax, testDat$x), length = gridRes)
yo <- seq(max(rmax, testDat$y), min(-rmax, testDat$y), length = gridRes)

interpV4 <- v4Interp(testDat, xo, yo, rmax, gridRes)

zo2 <- as.matrix(interpV4[,2:ncol(interpV4)])
xo2 <- matrix(rep(xo,length(yo)),nrow = length(xo),ncol = length(yo))
yo2 <- t(matrix(rep(yo,length(xo)),nrow = length(yo),ncol = length(xo)))
outsidecircle <- sqrt(xo2^2 + yo2^2) > 0.5
zo2[outsidecircle] <- 0

image(cond+xo*0.8,1.5+xo*0.8,zo2,zlim=c(0,0.45),col=colmatrix2,add=TRUE,useRaster=TRUE)

points(cond+montage$electrodelocs[blackelectrodes,1]*0.8,1.5+montage$electrodelocs[blackelectrodes,2]*0.8,pch=16,col="black",cex=1.5)

lines(cond+montage$headoutline[,1]*0.8,1.5+montage$headoutline[,2]*0.8,col="black",lwd=2)
lines(cond+montage$noseoutline[,1]*0.8,1.5+montage$noseoutline[,2]*0.8,col="black",lwd=2)
lines(cond+montage$Rearoutline[,1]*0.8,1.5+montage$Rearoutline[,2]*0.8,col="black",lwd=2)
lines(cond+montage$Learoutline[,1]*0.8,1.5+montage$Learoutline[,2]*0.8,col="black",lwd=2)

lines(c(cond,cond),c(0.05,0.95))
lines(c(cond-0.45,cond+0.45),c(0.5,0.5))

xvals <- Re(subjmeans[,cond])/5
yvals <- Im(subjmeans[,cond])/5
points(xvals+cond,yvals+0.5,pch=16,col=addalpha(nbdtpal[1],alpha=0.2),cex=0.5)

bdata <- data.frame(xvals,yvals)
D <- sqrt(mahalanobis(bdata, colMeans(bdata), cov(bdata)))
i <- which(D>3)
points(xvals[i]+cond,yvals[i]+0.5,pch=16,col=nbdtpal[2],cex=0.5)
points(mean(xvals[-i])+cond,mean(yvals[-i])+0.5,pch=16,col=nbdtpal[3],cex=1)
nexcluded[cond] <- length(i)
isincluded[i] <- 0
}

cleandata <- subjmeans[which(isincluded==1),]

contrastlevels <- c(0,2,4,8,16,32,64)

for (cond in 1:7){
  xvals <- Re(cleandata[,cond])/5
  yvals <- Im(cleandata[,cond])/5
  ellRot <- getel(data.frame(xvals,yvals))
  lines((ellRot[1, ]+mean(xvals))+cond, (ellRot[2, ]+mean(yvals))+0.5)

  text(cond,1,paste(contrastlevels[cond],'%',sep=''),adj=0.5,cex=1.5)
# totest <- data.frame(xvals,yvals)
# print(CI.test(totest))
}


# grouplabels <- as.factor(rep(1:7,each=89))
# participant <- as.factor(rep(1:89,times=7))
# a <- melt(cleandata[,1:7])
# simdata <- a[,3]
# withincirc <- anovacircR.test(data.frame(grouplabels,simdata,participant))

# for (cond in 2:7){
#   set1 <- data.frame(Re(cleandata[,1]),Im(cleandata[,1]))
#   set2 <- data.frame(Re(cleandata[,cond]),Im(cleandata[,cond]))
# print(tsqc.test(set1,set2,paired=TRUE))
#   
# colnames(set1) <- c('x','y')
# colnames(set2) <- c('x','y')
# fores <- rbind(set1,set2)
# mahala_sq <- pairwise.mahalanobis(fores, rep(1:2,each=89))
# D <- sqrt(mahala_sq$distance[1,2])
# print(D)
# }


```

With the outlier points removed, all seven conditions resulted in non-significant condition index tests (largest CI = 1.2, all _p_ > 0.19). A repeated measures $ANOVA^2_{circ}$ test was conducted, revealing a significant effect of stimulus contrast ($F_{(12,1056)}$ = 38.9, p < 0.001). Pairwise $T^2_{circ}$ statistics comparing the baseline (0% contrast) condition to each subsequent condition (Bonferroni corrected for 6 tests to $\alpha = 0.08$) revealed significant differences at 8% contrast ($T^2_{circ}$ = 0.32, $F_{(2,176)}$ = 28.4, D = 0.13, _p_ < 0.001), 16% contrast ($T^2_{circ}$ = 0.28, $F_{(2,176)}$ = 25.3, D = 0.19, _p_ < 0.001), 32% contrast ($T^2_{circ}$ = 0.10, $F_{(2,176)}$ = 8.5, D = 0.14, _p_ < 0.001) and 64% contrast ($T^2_{circ}$ = 0.34, $F_{(2,176)}$ = 35.3, D = 0.33, _p_ < 0.001). The study by Vilidaite et al. [-@Vilidaite2018] compared SSVEP responses between individuals with and without autism, as well as in a _Drosophila_ genetic model of developmental disorders. The raw data are available at: [http://dx.doi.org/10.17605/OSF.IO/Y4N5K](http://dx.doi.org/10.17605/OSF.IO/Y4N5K).

# Further considerations

It is worth pointing out that the statistical tests discussed in this paper are applicable only when the signal phase is expected to be consistent across observations. This is the case for most paradigms in which the nervous system is driven by a periodic stimulus. However, they are less obviously applicable to the analysis of endogenous neural oscillations and brain rhythms [@Berger1929; @Buzsaki2004], which will typically have random phase and broader bandwidths in the Fourier domain: other analysis methods have been developed for such signals [e.g. @Canolty2010]. When phases are consistent across repetitions, greater statistical power can be obtained by coherently averaging across repetitions to obtain a participant-level average [@Baker2021]. This practice is also necessary in order to use the multivariate methods discussed here, as the alternative is to discard the phase information and average amplitudes instead, rendering the data univariate.

The present paper has focussed on the Frequentist statistical tradition. However there are many advantages to the Bayesian approach, in which one can make direct quantitative comparisons of the evidence supporting both the experimental and null hypotheses [@Jeffreys1961]. Subject to determining appropriate priors, Bayes factor scores might be calculated for all of the statistics considered here, much as has been done previously for univariate T-tests [@Rouder2009] and ANOVA [@Rouder2017]. However this is a non-trivial undertaking, and is beyond the scope of the current paper.

Another possibility is to use machine learning techniques such as multivariate pattern analysis (MVPA) to analyse periodic data. This involvess training a classifier algorithm to distinguish between two (or more) experimental conditions or states, and then assessing classifier accuracy on fresh data. If different conditions produce distinct patterns of neural response, then classifier accuracy will be above chance. Such methods have been hugely influential in the fMRI literature [@Schwarzkopf2011], and for analysing event-related potential data collected using EEG or MEG [@Grootswagers2017]. However, they have not been widely applied to steady-state data (though see West et al. [-@West2015] for an example). In principle, the real and imaginary Fourier components can be treated as separate dependent variables, along with different recording locations and/or frequencies. This approach has the potential to offer sensitive, high-powered statistical tests that circumvent many of the shortcomings associated with traditional statistics.

Even when statistics are conducted using both the real and imaginary Fourier components, it is still typical to visualise the mean amplitudes. Several approaches to calculating appropriate error bars have been proposed. For example, Pei et al. [-@Pei2017] suggest calculating the nearest and farthest points from the origin on the bounding ellipse, and using these to derive standard errors for the amplitude. This approach is somewhat demanding, though we have implemented an _R_ function to calculate error bars using this method (available through the _getampCI_ function). However, we feel that bootstrap resampling offers a powerful and general alternative for calculating 95% confidence intervals on amplitudes. This is achieved by resampling the complex data (with replacement) and calculating a resampled complex mean. The amplitude is then derived for this resampled mean, and the procedure repeated a large number of times (1000 or 10000 repetitions is typical) to build up a population of resampled amplitudes. Upper and lower 95% confidence intervals on the amplitude can then be taken at appropriate quantiles of this population. 

# General recommendations for analysing periodic data

The simulations reported here allow us to make several recommendations for how periodic data should be analysed. Multivariate statistics should be used for phase-locked Fourier data instead of univariate statistics such as T-tests and ANOVA. This avoids problems from distributions of amplitudes violating the test assumptions, and also provides a sensitivity benefit from the inclusion of phase information. Outliers should be removed when they have a Mahalanobis distance exceeding 3, and the pairwise Mahanalobis distance reported as a measure of effect size. For sample sizes of N < 32, the $T^2_{circ}$ and $ANOVA^2_{circ}$ statistics can be used if the condition index test is non-significant for all conditions. Alternatively, the $T^2$ or MANOVA statistics should be used when these conditions are not met. The greater power afforded by these tests should in general lead to more accurate statistical inferences when analysing periodic data.

# References